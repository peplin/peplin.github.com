---
layout: post
title: August 23, 1966
permalink: /2011/05/august23/
---

How do you tell the universe's story as our story? Myth has been intertwining
the small and the large for a very long time, with varying degrees of success.
Our modern physical cosmology is another mythic story in the suite of human
cosmogonies. Though it holds a special place as one that is connected to
physical reality, it may be interpreted in many ways. By implementing mechanical
and digital interaction technologies, the interpretation can become
re-manifestation, where the observer tells a personal story of the cosmos.

[Proposal (PDF)](/files/august23/proposal.pdf)

![Logo](/images/august23/august_logo_transp.png)

From our proposal:

"We will also explore the possibility of a collaborative universe creation
computer game written in Java and [Processing](http://processing.org/). Based on
a true to life cosmic starting point, participants can manipulate galaxies and
change the laws of physics on the fly. The creator can allow players to catalog
and experiment with a shared universe (similar to the game, Spore ) while
exploring scientific laws and theories of creation, order, and design."

## Origin

I was thinking about how to bring our ideas back into one solid, rich project.
I was worried we would spend the semester fiddling our way somewhere, which
while I'm sure would be a worthwhile experience, makes me a little uneasy. I'm
a software person, and I like specifications.

## Abstract

Consider it a play off of the many-worlds theory - alongside our world, with
its crumbling economies and warring nations, there exists a digital universe
that is directed by you, the user. Just like here, many parameters are outside
of your control. The interesting part is choosing what you can, timing as you
may, and watching the results.

## Comments

* I see this as Second Universe ... no, Next Universe or SIM Verse, but better
* I'd also like to have a way to keep in touch with the full cosmos.  Perhaps we
    can have multiple levels of spacetime interaction: one person has built the
    cosmos, another a galaxy, within that another person has constructed a
    planetary system, etc.
* Our final display can be a walk-through installation where multiple people can
    customize and build their universes together, while others are viewing them
    on-screen.
* The ability of the user to tell their own story and let that story be
    reintegrated into a larger one is a very strong aspect of this, but this is
    not immediately apparent to the user: it's still Meta-data.  We need a mode
    of interaction that will "bridge" these scales.


## Names

* Simverse (probably troublesome, with the current trademarked Sim games)
* United Universe
* Origins
* ONE - Origins, now electronic: this over-emphasizes the digital part of our
    project.
* TWO - The Wormhole Opens, also two because of two universes, ours and the
    digital.
    * Twoverse
* Wormscale
* Your Verse
* ~Multi-Verse
* ~CosmUs

## Requirements

There are a few requirements that I think will unite our ideas and questions
(and I think there are many more yet to be explored).

It must be:

* Multiplayer
    * One or two primary terminals in DL-1, but also simultaneously active with
        online and mobile users
* Low commitment for a user, and won't allow you to devote too much time to it
    at any one moment
    * eg. 5 minutes per day to check up on things or make small changes
    * Can't sink 20 minutes, then get bored and never think about it again -
        want to live past the &amp;quot;wow, that's neat&amp;quot; factor
* Centralized in DL-1 with the primary interaction being there, where we can
    offer unique interfaces and better graphics
    * Offer something unique here to encourage users to come together in a
        physical space to talk about their creations or plan explorations and
        colonization pushes in the universe

## Software

* Google Sky modification
* Integration of S2plot and Processing
* There exist hi-res geometry converters that make this deployable in different
    environments, including planetaria
* Backend server written in Python or Java
    * Use MySQL database backend to store and centralize data about users and
        what they create
    * Server runs constantly, updating positions of objects and their behaviors
* Web-based frontend running on same server
    * Text-based status updates, some simple graphics to display orbits and
        distances
* Processing frontend, using OpenGL for graphics
    * Should be able to run on any computer that can connect to the server,
        either on campus or off

### User Accounts##

I think this game would require user accounts for the cooler features, so
objects could be owned by different people. We could figure out a way to label
them, but I don't think there needs to be a formal, competitive game. It's
everyone vs. space, and if someone builds drone satellites to smash into
someone else's launch vehicles in race to colonize, so be it!

## Input Elements

The universe may start with some phenomena and astral bodies, but the rest is
up to the users, who could add and determine.  The layers are:

### Man-made Bodies##

* Space travel vehicles
    * Size
    * Population
    * Speed
    * Radiation protection
    * Heat/cold protection
    * Power generation
    * Flight plan or orbit, could be a recurring vessel traveling from the moon
         to Earth
* Satellites
    * Purpose
    * Corporate/national logo, for a high-level sort of "colonization" of areas
        of galaxies
    * Speed
    * Orbit
    * Destination for long distance scientific missions

### Celestial Bodies##

* Cosmos?
* Group of Galaxies
    * Galaxies
    * gas between galaxies
* Galaxy
    * stars
    * black holes
    * interstellar gas
* Star system
    * planet
    * comet
* planetary system (including a mirror of our solar system)
* planet (network of cities)
    * Size
    * Density
    * Composition (desert, ice, dense gas, etc)
    * Initial orbit
    * Population and habitability
* Single city ( network of neighborhoods )

### Feedback and Interaction Systems##

The special installations in DL-1 can have control methods developed by our
team. The Wiimote would be a fantastic way of dragging and dropping new
elements into the universe, and you could drag onscreen sliders to change
parameters. A multitouch table would accomplish the same thing, and be that
much cooler. There is certainly a way to incorporate video feeds of people into
the game as well - as I said, i think we can incorporate all of our ideas here,
it's just a great way to integrate them all together.

* Multi-touch display interface: cheaply develop higher-dimensional,
    sophisticated haptics
    * multi-touch table
    * multi-touch upright panel
    * Faux 3D interface
* Lightbox, multi-person space
* Focus on Some larger connections that we want to make evident
    * cosmology as mythology (storytelling)
    * complexity (particles as swarms): include agent-based modeling
* Focus on non-visual sensory methods
    * what is the sound of space
    * What does infinity feel like?

### Integration of digital universe back into our physical world##

I haven't explored this much yet, but even cooler than representing astronomic
data would be representing data from our simulated universe. It would have some
personal connection to the users, making them that much more interested.

* Analog meter displays showing the number of spacecraft/asteroids in a certain
    area
* Color shifting light from green to red to indicate the number and severity of
    predicted interstellar collisions
* Unobtrusive sounds to indicate when a new planet or galaxy is born...ping!
* A physical sphere at different input points (kiosk, but not a kiosk) will
    relay a given star's features.
    * we can replay events on different timescales on this 3D viewer.
* How can we explicitly connect to other concepts and paradigms featured in our
    proposal, like complexity, emergence, the similarities between biology and
    cosmology?

## Features

### Real, live universe data integration##

Far from an isolation simulation, however, this parallel universe has some
crossover with our current space (and in real time!). Imagine the Sun in this
digital universe, matching the solar flares of our own. I see the entire
graphical interface going blurry/static-y during periods of high proton wind,
or cause your space vehicles and satellites to lose communication and spin out
of control.

We could also bring in factors here that have nothing to do with astronomy -
the amount of activity in DL-1 could determine properties of some special
galaxy cluster, or the people in the room could automatically have an asteroid
generated for them.

### Time scales##

Time is a big issue with this game. We want it to be slow enough that you can
make some decisions one day, but then you have to wait 24 hours or more for big
enough changes to happen. The time should also be fast enough that we can
demonstrate some really amazing phenomena in a more human time scale - note
that not all of the bodies have to obey the same time scale, since we're not
trying to be super fact-accurate. The time scale for stars could be greatly
accelerated so we can ultimately see them collapse. For space travel, maybe it
takes 3 hours instead of 3 days to go from Earth to the moon. Do we ever reset
the universe? This may be mediated by having several levels of God-power.  Each
level controls a different scope of the verse.

### Replayability##

We've discussed replayability before, and I think this game will satisfy our
requirement. There are many ways to lure users back:

* The fun of putting your name on a cluster of planets or on a convoy of space
    vessels destined for Andromeda.
* The parental like ownership you may feel watching your star grow up, and the
    pains in your heart when it dies (oh, how sad)
* Something to check on for just a few minutes each day, and more educationally
    rewarding than your fantasy football scores
    * There could be a great mobile aspect of this - I do want to make a web
        interface so you could check on a brief status of your objects from a
        web site, without being in or near DL-1. The system could also text
        message your phone if a collision is predicted between any of your
        objects (there's actually a really cool live data stream for this in the
        real world, which lists upcoming possible collisions ranked by proximity
        and probability).

### Dynamic Scale and Interaction##

All of this is dynamic, meaning that if someone came along and created a new
empty galaxy, another person could see that galaxy later and populate it with
their solar systems. There's a really fascinating, hierarchical structure to
the whole thing that makes it //ideal// for software development.

# Software Design

Here we have the "holy trinity" of our universe - God, Science, and the Loyal
Subjects


loyal subjects is interface?

## God (Java)

* System level responsibilities
    * Insert new objects into the Database from a queue of new objects, once per
        timestep
    * Change user parameters of existing objects directly in the Database
* User-level responsibilities
    * Accept requests from users via RPC or JMS
        * Graphical Client to God Messages
    * User notification (e-mail, SMS, etc)
    * Communicate with Science to get most up-to-date positioning info (rather
        than going through the Database)
        * God to Science Messages
    * Communicate with Science to give user modifications of existing objects -
        light enough to be on-the-fly, doesn't matter for simulation

## Science (C++)

* Cooperative, System level responsibilities
    * Store updates from God between commits to the Database
    * Communicate with God to coordinate commits to and updates from the
        Database
* Simulation level responsibilities
    * Calculate object position, velocity, acceleration, etc, continuously
    * Commit/update from the Database at some greater interval, ~10 minutes

## Reflection

The most important goal to me, and one we did accomplish, was to complete a
vertical slice of an entire system. Our gallery installation and the hardware
and software that support it touch an amazing number of topics, and I know
everyone on the team has learned a great deal about their own and one another's
fields in the process. I am extremely happy to have expanded my knowledge with
hands on experience in:

* XML-RPC
* Java Servlets
* Multi-threading and databases in Java
* Sound in the Processing environment
* Library development for Processing
* Releasing a software project online with an open source license
* Multitouch input processing
* Graphics performance optimization
* Analog signal processing (filters, opamps)

I am coming away from this semester having created multiple tools, and I am
eager to pass those on to anyone else with an interest in these topics. 

Our team used an online wiki (a [TiddlyWiki](http://www.tiddlywiki.com/)) to
organize our thoughts and coordinate the project plan. I was very pleased with
how this fit into the group's workflow. The wiki served as the default place to
put any thoughts or meeting notes, which greatly simplified the process of
sharing information.

# Documentation

## Multitouch Table

The multitouch table created for the project is a rear diffused illumination
design (see figure \ref{fig:mtdiagram}), and is housed in a core‐ten steel
cabinet with recessed cooling fans and an access panel on the rear vertical
wall.  The touch surface is a 1⁄2" polycarbonate sheet with an adhesive
projection film applied to the underside (acting as a diffuser for the projected
image).  

In addition, infrared light is also projected at the diffuser from below (inside
the cabinet) the touch surface. The table currently uses an array of six
multiple IR LED lamps. When an object touches the surface it reflects more light
than the diffuser or objects far away from the surface. The change in light is
detected by a web cam placed inside the cabinet, and the signal is fed to the
computer for analysis.

### Multitouch Software

The multitouch table uses [The Beta](http://ccv.nuigroup.com/), from the NUI
Group, to process the video stream from the webcam. The Beta, tbeta for short,
is an open source tool that analyzes a video to find tracking data for objects
it recognizes as fingers or cursor devices. The software provides a great deal
of control over the video parameters (high-pass filter, amplification,
threshold, etc.) that adapts well to many types of multitouch displays.

Tbeta outputs the tracking data using the [TUIO protocol](http://tuio.org/),
which is an open framework for receiving input events in various programming
environments. For this project, the TUIO events sent by tbeta were received
using the open source Java TUIO library in a Processing sketch. 

![MT-screenshot](/images/august23/Screenshot-MultitouchClient.png)
A screenshot of the Multitouch Client with a group of stars displayed at the
default zoom level. The lines connecting the stars are constellations.

![MT-screenshot](/images/august23/Screenshot-MultitouchClient-1.png)
A screenshot of the Multitouch Client viewing the details of a single star.

![MT-screenshot](/images/august23/Screenshot-MultitouchClient-2.png)
A screenshot of the Multitouch Client zoomed in for a closer look at a cluster
of stars.

## Pulse Oximiter (Heartbeat Monitor)

In researching ways to detect the human heartbeat, we found that
photoplethysmography would be the simplest and least expensive way to bring an
immediate and personal touch to the gallery installation. A photoplethysmograph
is usually obtained with a pulse oximeter - this is the same device that grips a
person's finger in hospitals to measure the heart & respiration rates. 

### Concepts

A pulse oximeter simply illuminates the skin with light from an LED (usually
infrared), and measures the luminance of the skin on the other side. Each
cardiac cycle brings more blood to the extremities, and thus the finger is
denser and less light passes through to the detector. When the blood flows away,
more light is let through. This fluctuation can be recorded and the timing of
the luminance peaks used for recording the heart rate.

This project required knowledge of
[photoplethysmography](http://en.wikipedia.org/wiki/Photoplethysmograph)
[signal processing filters](http://www.swarthmore.edu/NatSci/echeeve1/Ref/FilterBkgrnd/Filters.html)
and [operational amplifiers](http://web.telia.com/~u85920178/begin/opamp00.htm).

~[Pulse](/images/august23/schematic-final.png)

This schematic describes the circut used inside the pulse oximeter. It is
provided as a Fritzing file in the software package.

### Implementation

This device is advantageous for its low intrusiveness. In our implementation,
the visitor just needs to gently place their finger on top of the light sensor.
Depending on the person, the shape, rate and range of the photoplethysmograph
obtained can vary widely, but we found the results distinct enough to obtain a
heart rate from almost every participant. 

Our device used the amplified signal of an inexpensive photo-resistor passed
through a high pass filter and captured by an Arduino microcontroller
\cite{ARD}. The microcontroller fed an averaged luminosity to a Processing
\cite{P5} sketch on the host computer, which analyzed the signal for peaks. The
peaks were then converted to a frequency, and passed along to the gallery
software. The software is general to heart rate monitoring, and can be used for
other applications that are interested in the data.

The electronics schematics are included as both a PDF diagram and as a file for
[Fritzing](http://fritzing.org/) and can be found in this package at
\texttt{/august/doc/heartbeatmonitor}.

The source code for the microcontroller and for computer-side analysis can be
found in this package at \texttt{/august/src/gallery/PulseOximeter} and
\texttt{/august/src/gallery/WiremapClient} \cite{PACK}.

## Twoverse System & Library

The software core of the August 23, 1966 project is a Java software system
coined as "Twoverse," a parallel universe that exists only in the digital realm
with some key points of connection to the real world. It can be considered an
extension of the many-worlds interpretation - alongside our world, with its
crumbling economies and warring nations, there exists a digital universe that is
directed by you, the user. Just like here on Earth, many parameters are outside
of your control. The interesting part is choosing what you can, timing as you
may, and watching the results.

![Arch](/images/august23/twoverse_arch.jpg)

The Twoverse architecture can be split into three levels - server, client and
input. This diagram includes some unimplemented elements, such as input from a
sound sensor.

### System Design

The broad concept of Twoverse includes mechanics and partial system
specifications for a massively multiplayer online game. The scope was minimized
due to time constraints and to better fit the gallery installation. However,
even with a smaller scope, a complete vertical slice of the entire system was
implemented and used. Downsized from a universe of many types of objects, the
system currently supports a universe made of stars with a few properties, and
constellations that connect them with meta-objects known as links. Scalability
was stressed from the beginning of development, so extending the universe with
new objects will require a minimal amount of work.

### Server

The system relies on a central server to provide the following:

* A persistant database of all objects in the universe and their current state,
    using MySQL
* User account management, as well as authenticated session negotiation
* A public API for interacting with the universe, via Apache
    [XML-RPC](http://ws.apache.org/xmlrpc/)
* Client pull style updates for minimizing bandwidth requirements, via an XML
    feed
* A web-browser based frontend to view the status of objects in the universe,
    via PHP

### Client

Using the public API, many types of clients are possible. This includes
graphical, text-based, mobile, e-mail, etc. The clients implemented for the
gallery installation are graphical clients written using the Processing
development environment and include the following features:

* Users can scroll and zoom around a graphical universe of glowing stars
* Users can click on an individual star to view a close-up view and additional
    details about its creation and properties
* Users can create a new star in the universe, and watch a 3D visualization of
    their star's formation
* Users can draw constellations that connect the stars in the universe, and
    leave them for other users to see
* Users can visit the gallery website to view a table of all of the stars in the
    universe, their properties and current status

The graphical client uses the XML-RPC API as defined by the server, and updates
its local cache of the universe via the server's XML feed.

![Star Chart](/images/august23/starchart.png)

A screenshot of the star chart in a web browser during the time the gallery was
open.

### Library

The client and server for Twoverse share many common functions, and were
designed to inherit their source from the same tree of code. They both stem from
a Twoverse Java library, which includes many utility classes, shared
functionality and the server executable. The Twoverse library provides these
features and many others:

* Database wrapper class for a persistent universe - could be used to run SQL
    database client-side
* XML-RPC servlet for serving XML-RPC requests
* Thread-safe universal object manager for maintaining the state of the universe
* User session manager
* Small unit test suite for core classes
* Processing camera wrapper class for simplifying the elusive camera() function
* Flexible 2D/3D point coordinate class

The source code as well as an extensive HTML reference is included in the
software package at \texttt{/august/src/Twoverse/doc} \cite{PACK}.

See the project wiki \cite{WIKI} for more information on the motive for Twoverse
and possible future plans.

## Wiremap

The [Wiremap](http://wiremap.phedhex.com) is an innovative projection technique
that displays a 3D image in space using a standard computer projector. The
projector throws a beam of light on an array of vertical wires. From the focal
point of the projector’s lens, all the wires are evenly spaced from one another
and have a corresponding distance from the projector. With that information
(both a horizontal and depth coordinate), and using some careful calculations on
the computer, we can project simple images at various depths in the field. From
any perspective besides the projector position, the wires appear randomly placed
and the image becomes visible.

Our implementation uses mason's string for the field, 3/4" plywood for the top
and bottom alignment/hanging boards, and standard nuts and washers as anchors
for each string. Our map has 256 strings, placed in a randomized dimension of
depth through an equal number of holes in both the top and bottom alignment
boards. The strings are secured with bolts on the top board and weighted down
with a washer below the bottom board. When the top board is raised to 8ft, the
wires become taught and can be aligned to a 90 degree angle with the floor. The
Wiremap must be calibrated each time the projector is positioned - this includes
making sure the wires are parallel, the projector sees the wires evenly spaced,
and there is no unnecessary tilt or keystone in the projector's image.

![Wiremap](/images/august23/wiremapconst6.jpg)

The 256-string Wiremap installed in the gallery.

### Wiremap Software Library

In order to facilitate quicker prototyping and make the Wiremap software more
accessible to the team, we wrote a simple Processing library for rendering
certain shapes in the Wiremap field. The library replaces the source code
provided by the creator of the Wiremap \cite{AH} by reducing code duplication
and abstracting most of the implementation details away from a user who wishes
to simply draw a sphere, rectangle or sliver in the field. The library also
includes a novel calibration method developed in response to inaccuracies in our
Wiremap.

### Abstraction

The Wiremap library gathers the coordinate conversion and wire selection math
into a single class. The previous method required duplicating a set of functions
in every Processing sketch that output to the Wiremap. Now, the user creates an
instance of the Wiremap class and provides a few key measurements of the
physical interface as well as a text file listing the wire depths. The
calculation is done as necessary, and not exposed to the user.

### Coordinate Systems

One key difference between the original source and the Wiremap library is the
coordinate system used for each plane. Previously, the coordinates of X, Y and Z
were all physical inches and matched the actual dimensions of the Wiremap. To
facilitate quicker transitioning from a regular Processing sketch (using the
standard 2D renderer) to one for the Wiremap, the X and Y were changed to be in
the standard, Processing-style pixel coordinate system.

The Z plane remains in inches, as there is no obvious relationship between Z
space on the screen (which is infinite in both directions) and Z space in the
Wiremap field (limited by the physical dimensions). Thus, Z coordinates in the
field range from 0 to the field depth.

The library has been released under the Apache open source license, and will
continue to evolve after this project's completion. See the library's
documentation for details on installation and usage \cite{CP.


# Gallery Installation

![Gallery](/images/august23/gallery_installation.png)

To replicate the gallery installation, the following systems & equipment are
required:

### Server

* MySQL server
* Web server, root pointing to $<$location of august package$>$/www
* Ports 80 & 8080 open
* Java JRE 1.5 or greater

### Multitouch or Desktop Client

* Multitouch table
* Hardware accelerated graphics
* Network connection to server (can be over Internet)
* Sound output (ideally to a radio headset)

### Wiremap Client

* Wiremap
* Projector
* Sound output (ideally to a sound dome)

#### Server Setup

* Create an "august" user for the MySQL database
* Initialize the august database using the SQL script found at
    \texttt{/august/database/create.sql}
* Run the compiled Twoverse server to check the database connection and create
    the XML feed file: \texttt{java -jar twoverse\_server.jar}
* Create a symbolic link to the feed.xml file that was just created in the web
    server's root directory (this should be /august/www/)

#### Multitouch or Desktop Client Setup

* Plug the radio headset into the audio output of the computer (if you are using
    a radio)
* Open the client sketch (either Multitouch or Desktop version) and modify the
    configuration parameter for the server IP to point to the correct address.
* Configure the Wiremap server IP configuration parameter in the CreateMode.pde
    file to point to the address of the Wiremap client (if you are using one)
* Run the sketch in Processing, or export a compiled application to simplify
    restarting the application

#### Wiremap Client Setup

* Plug the sound dome into the audio output of the computer (if you are using a
    sound dome)
* Open the client sketch and modify the configuration parameter for the
    Multitouch Client IP to point to the correct address of the Multitouch
    Client (not the Twoverse server)
* Run the sketch in Processing, or export a compiled application to simplify
    restarting the application

The Multitouch or Desktop client connects to the Twoverse server and manages the
creation and storing of new stars. The Wiremap client receives a message from
the Multitouch Client when a new star is being created, which initializes the
sound and visuals for the star formation experience. See the Twoverse library
documentation for more detailed information on this relationship.

## References

* [Wiremap Shapes Library](https://github.com/peplin/wiremap-shapes)
* [Wiremap](http://wiremap.phedhex.com)
